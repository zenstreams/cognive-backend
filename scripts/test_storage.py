#!/usr/bin/env python3
"""
MinIO Storage Test & Demo Script

This script demonstrates all storage operations including:
- File uploads/downloads
- Object CRUD operations
- Presigned URL generation
- Bucket listing
- Lifecycle policy validation

Usage:
    python scripts/test_storage.py
"""

import asyncio
import io
import logging
import sys
import tempfile
from pathlib import Path

# Add parent directory to path for imports
sys.path.insert(0, str(Path(__file__).parent.parent))

from app.core.storage import BUCKETS, get_storage_client, init_storage

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
)
logger = logging.getLogger(__name__)


async def test_file_operations():
    """Test file upload and download operations."""
    logger.info("\n" + "=" * 60)
    logger.info("TEST: File Upload/Download Operations")
    logger.info("=" * 60)

    storage = get_storage_client()

    # Create a temporary test file
    with tempfile.NamedTemporaryFile(mode="w", delete=False, suffix=".txt") as f:
        test_file = f.name
        f.write("This is a test file for Cognive storage!\n")
        f.write("MinIO is working correctly.\n")

    try:
        # Test 1: Upload file
        bucket = "agent-artifacts"
        object_key = "tests/sample.txt"
        
        logger.info(f"Uploading file to s3://{bucket}/{object_key}")
        storage.upload_file(
            bucket_name=bucket,
            object_name=object_key,
            file_path=test_file,
            metadata={"test": "true", "author": "test-script"},
        )
        logger.info("‚úÖ File uploaded successfully")

        # Test 2: Download file
        download_path = tempfile.mktemp(suffix=".txt")
        logger.info(f"Downloading file to {download_path}")
        storage.download_file(bucket, object_key, download_path)
        
        # Verify content
        with open(download_path, "r") as f:
            content = f.read()
            assert "Cognive storage" in content
        logger.info("‚úÖ File downloaded and verified")

        # Test 3: Generate presigned URL
        logger.info("Generating presigned URL...")
        url = storage.get_presigned_url(bucket, object_key, expiration=300)
        logger.info(f"‚úÖ Presigned URL: {url[:100]}...")

        # Cleanup
        storage.delete_object(bucket, object_key)
        Path(test_file).unlink()
        Path(download_path).unlink()
        logger.info("‚úÖ Cleanup completed")

    except Exception as e:
        logger.error(f"‚ùå File operations test failed: {e}")
        raise


async def test_object_operations():
    """Test direct object operations (bytes)."""
    logger.info("\n" + "=" * 60)
    logger.info("TEST: Direct Object Operations")
    logger.info("=" * 60)

    storage = get_storage_client()
    bucket = "report-exports"
    
    # Test 1: Put object (bytes)
    test_data = b"Sample report data\nGenerated by Cognive\n2024-01-01"
    object_key = "tests/report.txt"
    
    logger.info(f"Putting object to s3://{bucket}/{object_key}")
    storage.put_object(bucket, object_key, test_data)
    logger.info("‚úÖ Object uploaded")

    # Test 2: Get object (retrieve bytes)
    logger.info("Retrieving object...")
    retrieved_data = storage.get_object(bucket, object_key)
    assert retrieved_data == test_data
    logger.info("‚úÖ Object retrieved and verified")

    # Test 3: List objects
    logger.info("Listing objects with prefix 'tests/'...")
    objects = storage.list_objects(bucket, prefix="tests/")
    assert object_key in objects
    logger.info(f"‚úÖ Found {len(objects)} objects")
    for obj in objects:
        logger.info(f"   - {obj}")

    # Cleanup
    storage.delete_object(bucket, object_key)
    logger.info("‚úÖ Cleanup completed")


async def test_bucket_operations():
    """Test bucket-level operations."""
    logger.info("\n" + "=" * 60)
    logger.info("TEST: Bucket Operations")
    logger.info("=" * 60)

    storage = get_storage_client()

    # List all buckets
    logger.info("Listing all buckets...")
    buckets = storage.minio_client.list_buckets()
    logger.info(f"‚úÖ Found {len(buckets)} buckets:")
    for bucket in buckets:
        logger.info(f"   - {bucket.name} (created: {bucket.creation_date})")

    # Verify expected buckets
    bucket_names = {b.name for b in buckets}
    for expected_bucket in BUCKETS.keys():
        if expected_bucket in bucket_names:
            logger.info(f"‚úÖ Bucket '{expected_bucket}' exists")
        else:
            logger.error(f"‚ùå Bucket '{expected_bucket}' missing!")

    # Test bucket policies
    logger.info("\nChecking lifecycle policies...")
    for bucket_name in BUCKETS.keys():
        try:
            lifecycle = storage.minio_client.get_bucket_lifecycle(bucket_name)
            logger.info(f"‚úÖ Lifecycle policy configured for '{bucket_name}'")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è  No lifecycle policy for '{bucket_name}': {str(e)[:50]}")


async def test_large_file():
    """Test large file upload (simulate real-world usage)."""
    logger.info("\n" + "=" * 60)
    logger.info("TEST: Large File Upload")
    logger.info("=" * 60)

    storage = get_storage_client()
    bucket = "audit-logs-archive"

    # Generate a 1MB test file
    size_mb = 1
    logger.info(f"Generating {size_mb}MB test file...")
    test_data = b"x" * (1024 * 1024 * size_mb)
    
    object_key = "tests/large-file.bin"
    logger.info(f"Uploading to s3://{bucket}/{object_key}")
    
    try:
        storage.put_object(bucket, object_key, test_data)
        logger.info(f"‚úÖ Uploaded {size_mb}MB successfully")

        # Verify
        retrieved = storage.get_object(bucket, object_key)
        assert len(retrieved) == len(test_data)
        logger.info("‚úÖ Download and size verification passed")

        # Cleanup
        storage.delete_object(bucket, object_key)
        logger.info("‚úÖ Cleanup completed")

    except Exception as e:
        logger.error(f"‚ùå Large file test failed: {e}")
        raise


async def test_use_cases():
    """Demonstrate real-world use cases."""
    logger.info("\n" + "=" * 60)
    logger.info("DEMO: Real-World Use Cases")
    logger.info("=" * 60)

    storage = get_storage_client()

    # Use Case 1: Store audit log
    logger.info("\n1Ô∏è‚É£  Use Case: Store Audit Log")
    audit_log = {
        "timestamp": "2024-01-01T10:00:00Z",
        "user_id": "user_123",
        "action": "agent.created",
        "agent_id": "agent_456",
        "metadata": {"framework": "langgraph"},
    }
    audit_data = str(audit_log).encode()
    storage.put_object("audit-logs-archive", "logs/2024/01/01/10-00-00.json", audit_data)
    logger.info("‚úÖ Audit log archived")

    # Use Case 2: Store agent execution replay data
    logger.info("\n2Ô∏è‚É£  Use Case: Store Execution Replay Data")
    replay_data = {
        "run_id": "run_789",
        "agent_id": "agent_456",
        "steps": [
            {"step": 1, "action": "llm_call", "model": "gpt-4"},
            {"step": 2, "action": "tool_use", "tool": "search"},
        ],
    }
    replay_bytes = str(replay_data).encode()
    storage.put_object("execution-replay-data", "runs/2024/01/run_789.json", replay_bytes)
    logger.info("‚úÖ Execution replay data stored")

    # Use Case 3: Generate report and store
    logger.info("\n3Ô∏è‚É£  Use Case: Store Generated Report")
    report_content = "COST REPORT\n" + "=" * 40 + "\nTotal Cost: $123.45\nAgent: customer-bot\n"
    storage.put_object("report-exports", "reports/2024/01/cost-report.txt", report_content.encode())
    
    # Generate shareable link
    url = storage.get_presigned_url("report-exports", "reports/2024/01/cost-report.txt", expiration=3600)
    logger.info(f"‚úÖ Report stored and presigned URL generated")
    logger.info(f"   Share link: {url[:80]}...")

    # Use Case 4: Store agent artifact
    logger.info("\n4Ô∏è‚É£  Use Case: Store Agent Artifact")
    artifact = "Agent output: Customer query resolved successfully."
    storage.put_object("agent-artifacts", "artifacts/agent_456/output_001.txt", artifact.encode())
    logger.info("‚úÖ Agent artifact stored")

    # List all test objects
    logger.info("\nListing all test objects:")
    for bucket_name in BUCKETS.keys():
        objects = storage.list_objects(bucket_name)
        if objects:
            logger.info(f"\nüì¶ {bucket_name}:")
            for obj in objects[:5]:  # Show first 5
                logger.info(f"   - {obj}")

    # Cleanup
    logger.info("\nCleaning up test objects...")
    cleanup_prefixes = ["logs/", "runs/", "reports/", "artifacts/", "tests/"]
    for bucket_name in BUCKETS.keys():
        for prefix in cleanup_prefixes:
            objects = storage.list_objects(bucket_name, prefix=prefix)
            for obj in objects:
                storage.delete_object(bucket_name, obj)
    logger.info("‚úÖ Cleanup completed")


async def main():
    """Run all tests."""
    logger.info("\n" + "=" * 60)
    logger.info("COGNIVE STORAGE TEST SUITE")
    logger.info("=" * 60)

    try:
        # Initialize storage
        logger.info("\nInitializing storage...")
        await init_storage()

        # Run test suites
        await test_bucket_operations()
        await test_file_operations()
        await test_object_operations()
        await test_large_file()
        await test_use_cases()

        logger.info("\n" + "=" * 60)
        logger.info("‚úÖ ALL TESTS PASSED")
        logger.info("=" * 60)
        logger.info("\nStorage is fully functional and ready for production use!")
        logger.info("\nüìä Summary:")
        logger.info(f"   - Buckets: {len(BUCKETS)}")
        logger.info(f"   - Endpoint: http://localhost:9002 (API)")
        logger.info(f"   - Console: http://localhost:9003 (Web UI)")
        logger.info("\nüéâ MinIO setup complete!")

    except Exception as e:
        logger.error(f"\n‚ùå TEST SUITE FAILED: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())

